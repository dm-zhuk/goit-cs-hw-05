from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict

import string
import requests
import matplotlib.pyplot as plt
import logging


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)


def get_text(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –ø–æ–º–∏–ª–∫–∏ HTTP
        return response.text
    except requests.RequestException as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É: {e}")
        return None


# –í–∏–¥–∞–ª–µ–Ω–Ω—è –∑–Ω–∞–∫—ñ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—ó —Ç–∞ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –≤ –Ω–∏–∂–Ω—ñ–π —Ä–µ–≥—ñ—Å—Ç—Ä
def clean_txt(text):
    text = text.translate(str.maketrans("", "", string.punctuation))
    return text.lower()


def map_function(word):
    return word, 1


def shuffle_function(mapped):
    shuffled = defaultdict(list)
    for key, value in mapped:
        shuffled[key].append(value)
    return shuffled


def reduce_function(key_values):
    key, values = key_values
    return key, sum(values)


# –í–∏–∫–æ–Ω–∞–Ω–Ω—è MapReduce
def map_reduce(text, num_workers=8):
    cleaned_text = clean_txt(text)
    words = cleaned_text.split()

    # 1. MAP: –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π –ú–∞–ø—ñ–Ω–≥
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        mapped = list(executor.map(map_function, words))

    # 2. SHUFFLE
    shuffled = shuffle_function(mapped)

    # 3. REDUCE: –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ —Ä–µ–¥—É–∫—Ü—ñ—è
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        reduced = list(executor.map(reduce_function, shuffled.items()))

    return dict(reduced)


# –§—ñ–ª—å—Ç—Ä: —Ç—ñ–ª—å–∫–∏ —Å–ª–æ–≤–∞ –∑ 6+ –±—É–∫–≤
def filter_by_length(word_counts, min_length=6):
    return {word: cnt for word, cnt in word_counts.items() if len(word) >= min_length}


# –í–∏–¥–æ–±—É–≤–∞–Ω–Ω—è —Ç–æ–ø-N —Å–ª—ñ–≤ (Top 10 most frequent words)
def top_words(word_counts, top_n=10, min_length=6):
    filtered_counts = filter_by_length(word_counts, min_length)
    return sorted(filtered_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]


def visualize_top_words(top_words, title="10 Most Frequent words"):
    words, counts = zip(*top_words)

    plt.figure(figsize=(10, 6))
    y_pos = range(len(words))
    plt.barh(y_pos, counts, align="center", color="#1f77b4")
    plt.yticks(y_pos, words)
    plt.gca().invert_yaxis()
    plt.xlabel("Frequency")
    plt.title(title)
    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    # –í—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–æ–±–∫–∏
    url = "https://gutenberg.net.au/ebooks06/0604171.txt"

    logger.info("‚è≥ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É ‚Ä¶")
    text = get_text(url)
    if not text:
        raise SystemExit("‚ùå –ü–æ–º–∏–ª–∫–∞: –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –≤—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç.")

    logger.info("‚è≥ –û—á–∏—â–µ–Ω–Ω—è –∑–Ω–∞–∫—ñ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—ó ‚Ä¶")
    cleaned = clean_txt(text)

    logger.info("‚è≥ –í–∏–∫–æ–Ω–∞–Ω–Ω—è Map-Reduce ‚Ä¶")
    counts = map_reduce(cleaned, num_workers=12)

    logger.info("‚è≥ –í–∏–¥–æ–±—É–≤–∞–Ω–Ω—è —Ç–æ–ø-10 ‚Ä¶")
    top10 = top_words(counts, top_n=10)

    logger.info("10 Most Frequent words:")
    for word, cnt in top10:
        logger.info(f"  {word:<15} {cnt}")

    logger.info("‚è≥ –ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞ ‚Ä¶")
    visualize_top_words(top10, title="üèÜ Top 10 most frequent words")
